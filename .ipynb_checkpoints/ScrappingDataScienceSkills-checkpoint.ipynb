{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Advanced Example\n",
    "-----------------------------\n",
    "---------------------------\n",
    "#Idea By [Jesse Steinweg-Woods](https://jessesw.com/Data-Science-Skills/)\n",
    "----------------------------------------------------\n",
    "\n",
    "#Reference CS109\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Data Science Skills\n",
    "=======================\n",
    "- What Skills are in demand for data scientist\n",
    "\n",
    "Scraping pages of indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the imports are case sensitive in python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import time\n",
    "import socket\n",
    "from IPython.display import HTML\n",
    "import bs4\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.indeed.co.in/jobs?q=Data+Science&l=\"\n",
    "\n",
    "source = urllib.request.urlopen(url).read()\n",
    "# source\n",
    "\n",
    "bs_tree = bs4.BeautifulSoup(source)\n",
    "# print(bs_tree.prettify())\n",
    "\n",
    "job_postings = bs_tree.find(id = \"searchCount\").contents[0]\n",
    "# job_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search yeilded 15,554 hits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15554"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The search yeilded %s hits\" % (job_postings).split()[3])\n",
    "# job_postings\n",
    "job_postings = int(job_postings.split()[3].translate({ord(','): None}))\n",
    "job_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556\n",
      "1546\n",
      "1536\n",
      "1526\n",
      "1516\n",
      "1506\n",
      "1496\n",
      "1486\n",
      "1476\n",
      "1466\n",
      "1456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c0dc3e7fba41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjobIDs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mjobLinks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/rc/clk?jk=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s jobs are there\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobLinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_pages = int(np.ceil(job_postings/10.0))\n",
    "jobLinks = []\n",
    "\n",
    "for i in range(num_pages):\n",
    "    if i%10 == 0:\n",
    "        print(num_pages - i)\n",
    "    url = \"https://www.indeed.co.in/jobs?q=data+scientist&l=\" + str(i*10)\n",
    "    htmlPage = urllib.request.urlopen(url).read()\n",
    "    bs_tree = bs4.BeautifulSoup(htmlPage)\n",
    "    jobLinksArea = bs_tree.find(id='resultsCol')\n",
    "    jobPostings = jobLinksArea.findAll('div')\n",
    "    jobPostings = [div for div in jobPostings if not div.get('Class') is None and ''.join(div.get('class')) == 'rowresult']\n",
    "    jobIDs = [jp.get('data-jk') for jp in jobPostings]\n",
    "    \n",
    "    #Form links to scrape skills from the job postings\n",
    "    for id in jobIDs:\n",
    "        jobLinks.append(url + \"/rc/clk?jk=\" + id)\n",
    "    time.sleep(1)\n",
    "        \n",
    "print(\"%s jobs are there\" % (len(jobLinks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped links\n",
    "with open('scraped_links.pkl', 'wb') as f:\n",
    "   pickle.dump(jobLinks, f)\n",
    "    \n",
    "# Read canned scraped links\n",
    "with open('scraped_links.pkl', 'rb') as f:\n",
    "    jobLinks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set = {'mapreduce': 0, 'spark': 0}\n",
    "\n",
    "## write initialization into a file, so we can restart later\n",
    "with open('scraped_links_restart.pkl', 'wb') as f:\n",
    "   pickle.dump((skill_set, 0),f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below does the trick, but could be optimized for speed if necessary\n",
    "# e.g. skills are typically listed at the end of the webpage\n",
    "# might not need to split/join the whole webpage, as we already know\n",
    "# which words we are looking for \n",
    "# and could stop after the first occurance of each word\n",
    "#the STRING opcode argument must be quoted {Error} \n",
    "original = \"data/scraped_links_restart.pkl\"\n",
    "destination = \"data/word_data_unix.pkl\"\n",
    "\n",
    "content = ''\n",
    "outsize = 0\n",
    "with open(original, 'rb') as infile:\n",
    "    content = infile.read()\n",
    "with open(destination, 'wb') as output:\n",
    "    for line in content.splitlines():\n",
    "        outsize += len(line) + 1\n",
    "        output.write(line + str.encode('\\n'))\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "\n",
    "with open('data/word_data_unix.pkl', 'rb') as f:\n",
    "    skill_set, index = cPickle.load(f)\n",
    "    print(skill_set,index)\n",
    "    print (\"How many websites still to go? \", len(job_links) - index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
